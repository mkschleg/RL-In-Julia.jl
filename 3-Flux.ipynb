{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flux for ML/RL\n",
    "\n",
    "This notebooks is going to lay out some basics about [Flux.jl](https://fluxml.ai). Unfortunately, flux currently takes quite awhile to precompile and load. This is being worked on, but I recommend evaluating the next cell before digging into the text which appears after. This will speed up after the first time as there will be cached pre-compiled version of the library (much like Plots from before).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Random, Statistics, BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Flux?\n",
    "\n",
    "Flux is a deep learning framework that uses source-to-source automatic differentiation through Zygote.jl. The resulting library is incredibly flexible and can deferintiate through many Julia functions right out of the box. The benefit of this is that _all_ of Flux's models are written in pure julia (even GPU operations!!), and the library can take full advantage of multiple dispatch. We will discuss the nice features which come from this down the road, but first lets start with a simple example (artificial regression with a linear model). We can then move to talk about how Flux can be used in RL research.\n",
    "\n",
    "\n",
    "Because our problem is artificial, we will need to create a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(10293)\n",
    "\n",
    "train_points = 2^14\n",
    "val_points = 2^9\n",
    "feature_size = 10\n",
    "ϵ = 0.01f0\n",
    "\n",
    "target_model = Chain(Dense(feature_size, 256, relu), Dense(256, 1)) # These layers default to using the global random seed!\n",
    "\n",
    "X_train = randn(Float32, feature_size, train_points)\n",
    "Y_train = target_model(X_train) + ϵ*randn(Float32, train_points)'\n",
    "\n",
    "X_val = randn(Float32, feature_size, val_points)\n",
    "Y_val = target_model(X_val) + ϵ*randn(Float32, val_points)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the dataset created, we will setup a model and do a simple training loop with mini-batch gradient descent. We will decompose some of the flux primitives afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "opt = Descent(0.01)\n",
    "\n",
    "model = Chain(Dense(feature_size, 64, relu), Dense(64, 1))\n",
    "loss(x, y) = Flux.mse(model(x), y)\n",
    "\n",
    "println(\"Initial:\")\n",
    "@show loss(X_train, Y_train)\n",
    "@show loss(X_val, Y_val)\n",
    "println()\n",
    "\n",
    "for n ∈ 1:100\n",
    "    train_loader = Flux.Data.DataLoader(X_train, Y_train, batchsize=batchsize, shuffle=true)\n",
    "    Flux.train!(\n",
    "        loss, Flux.params(model), train_loader, opt)\n",
    "    if (n) % 10 == 0\n",
    "        println(\"Epoch: $(n)\")\n",
    "        @show loss(X_train, Y_train)\n",
    "        @show loss(X_val, Y_val)\n",
    "        println()\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loop\n",
    "\n",
    "The first piece we need to decompose is the training loop. In the above example we are using Flux's built in `train!` function. The beauty of Julia and Flux is that this is written all using Julia (meaning we can customize our training loop w/o any extra computational cost). While not as useful for the purposes of ML, for RL this is a critical component as the training loop contains interactions with the environment and other various processing book keeping ideas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cust_train!(loss::Function, m, ps, data, opt)\n",
    "    for d in data\n",
    "        gs = gradient(ps) do\n",
    "            training_loss = loss(m, d...)\n",
    "            # Insert what ever code you want here that needs Training loss, e.g. logging\n",
    "            return training_loss\n",
    "        end\n",
    "        # insert what ever code you want here that needs gradient\n",
    "        # E.g. logging with TensorBoardLogger.jl as histogram so you can see if it is becoming huge\n",
    "        Flux.Optimise.update!(opt, ps, gs)\n",
    "        # Here you might like to check validation set accuracy, and break out to do early stopping\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layer\n",
    "\n",
    "Just like the training loop, all of Flux's layers are written in Julia. Below is an example of the a Dense layer, but there are plenty of other examples and layers (all written in Julia) found [here](https://github.com/FluxML/Flux.jl/tree/master/src/layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct CustDense{S, B, F}\n",
    "    W::S\n",
    "    b::B\n",
    "    σ::F\n",
    "end\n",
    "\n",
    "CustDense(W, b) = CustDense(W, b, identity)\n",
    "\n",
    "function CustDense(in::Integer, out::Integer, σ = identity;\n",
    "               initW = Flux.glorot_uniform, initb = Flux.zeros)\n",
    "    return CustDense(initW(out, in), initb(out), σ)\n",
    "end\n",
    "\n",
    "(l::CustDense)(X) = l.σ.(l.W*X .+ l.b)\n",
    "Flux.@functor CustDense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Optimiser\n",
    "\n",
    "Again, there are plenty of Optimisers in [Flux](https://fluxml.ai/Flux.jl/stable/training/optimisers/) but there is ample ability to define and create your own which will have little to no extra overhead. You can model complex optimisers from the already implemented optimisers in the code base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct CustDescent\n",
    "  eta::Float64\n",
    "end\n",
    "\n",
    "CustDescent() = CustDescent(0.1)\n",
    "\n",
    "function Flux.Optimise.apply!(o::CustDescent, x, Δ)\n",
    "  Δ .*= o.eta\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now we can put together these custom peices into something that looks almost exactly like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_model = Chain(CustDense(feature_size, 64, relu), CustDense(64, 1))\n",
    "opt = CustDescent(0.01)\n",
    "\n",
    "println(\"Initial:\")\n",
    "@show Flux.mse(cust_model(X_train), Y_train)\n",
    "@show Flux.mse(cust_model(X_val), Y_val)\n",
    "println()\n",
    "\n",
    "for n ∈ 1:100\n",
    "    train_loader = Flux.Data.DataLoader(X_train, Y_train, batchsize=batchsize, shuffle=true)\n",
    "    cust_train!(cust_model, Flux.params(cust_model), train_loader, opt) do m, X, Y\n",
    "        Flux.mse(m(X), Y)\n",
    "    end\n",
    "    if (n) % 10 == 0\n",
    "        println(\"Epoch: $(n)\")\n",
    "        @show Flux.mse(cust_model(X_train), Y_train)\n",
    "        @show Flux.mse(cust_model(X_val), Y_val)\n",
    "        println()\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-up\n",
    "\n",
    "Flux is a flexible yet powerful library for working with all types of deep networks, which makes it ideal for research code! There are some rough edges (that are being actively worked on), but the whole project is open source and written in a single language (unlike many other popular packages). With the above examples you should be able to start digging into your own custom models and optimizers!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
