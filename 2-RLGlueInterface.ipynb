{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Interface\n",
    "\n",
    "There are countless ways to setup the interface between the agent and its environment. In this tutorial, I advocate for the interface developed by Brian Tanner, Adam White, and Rich Sutton through RLGlue. I've been using this basic framework for an interface for some time, and have always found it to align well with the RL problem and helps reduce error in managing what the agent can and cannot see about the environment.\n",
    "\n",
    "## RLGlue\n",
    "\n",
    "Many people will be familiar with the OpenAI Gym API for building the interface of agents and their world. While this is easily doable in Julia (and a might more convenient than the python interface due to multiple dispatch), I am a user and believer of the RLGlue interface developed by Brian Tanner and Adam White while at the University of Alberta. This interface is usable for all sorts of algorithms, paradigms, and experiments and does not tie the user into just control or just prediction. As my research tends to straddle both sides, you can see why I would be wanting a truely flexible framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "\n",
    "Lets start with the world. The interface is easy enough\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type AbstractEnvironment end\n",
    "\n",
    "function start! end\n",
    "function step! end\n",
    "\n",
    "function get_state end\n",
    "function get_reward end\n",
    "function is_terminal end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is a really basic interface, where the user has total control over what start, and step return. While this may be beneficial in some instances, we often want our environments to accept and return a consistent set of information. Specifically, we want our agents to accept an action and return the resulting transition tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function start!(env::AbstractEnvironment, args...)\n",
    "    _start!(env, args...)\n",
    "    return get_state(env)\n",
    "end\n",
    "\n",
    "\n",
    "function step!(env::AbstractEnvironment, action, args...)\n",
    "    _step!(env, action, args...)\n",
    "    return get_state(env), get_reward(env), is_terminal(env)\n",
    "end\n",
    "\n",
    "\n",
    "function _start! end\n",
    "function _step! end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when a user is implementing a type of AbstractEnvironment, they have to implement the `_start!`, `_step!`, `get_state`, `get_reward`, and `is_terminal` functions and they can use start! and step! to ensure similarity in return structure. You may be noticing the `args...` parameters signifying a variable number of args in the function call. While not critical, these can help the user define custom behavior that isn't implemented in the basic functions here. For instance, say a user wanted to maintain their own random number generator (which I am often known to do if I'm taking advantage of Julia's threading (pre 1.3) to do experiments).\n",
    "\n",
    "\n",
    "Now that we have a basic interface built. Lets do a simple example on how you could implement a simple Markov chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    MarkovChain(num_states)\n",
    "\"\"\"\n",
    "\n",
    "mutable struct MarkovChain <: AbstractEnvironment\n",
    "    size::Int # The size of the markov chain\n",
    "    state::Int\n",
    "    MarkovChain(size) = new(size, size ÷ 2) # the ÷ symbol throws away the remainder.\n",
    "end\n",
    "\n",
    "function _start!(env::MarkovChain)\n",
    "    env.state = env.size ÷ 2\n",
    "    range = ((env.size ÷ 2) - env.size ÷ 4):((env.size ÷ 2)+(env.size ÷ 4))\n",
    "    env.state = rand(range)\n",
    "end\n",
    "\n",
    "function _step!(env::MarkovChain, action)\n",
    "    \n",
    "    if action == 1 # LEFT\n",
    "        env.state -= 1\n",
    "    elseif action == 2 # RIGHT\n",
    "        env.state += 1\n",
    "    else\n",
    "        throw(\"Error\")\n",
    "    end\n",
    "end\n",
    "\n",
    "get_actions(env::MarkovChain) = (1, 2) # Remember julia starts indexing at 1!\n",
    "\n",
    "get_state(env::MarkovChain) = env.state\n",
    "get_reward(env::MarkovChain) = env.state == env.size ? 1.0 : -1.0\n",
    "is_terminal(env::MarkovChain) = env.state == env.size || env.state == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a simple environment API, and a Markov Chain to play with. Lets do a quick simulation to make sure things are working correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MarkovChain(10)\n",
    "states = Int[]\n",
    "s = start!(env)\n",
    "push!(states, s)\n",
    "\n",
    "while !is_terminal(env)\n",
    "    s, r, t = step!(env, rand(get_actions(env)))\n",
    "    push!(states, s)\n",
    "end\n",
    "\n",
    "@show states;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "Now we need to move on to building an agent that will act in the environment. We are going to follow the same idea as in the environment to implement a basic tabular QLearning agent (if you need a review see [Rich's RL Book](http://incompleteideas.net/book/the-book-2nd.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type AbstractAgent end\n",
    "\n",
    "mutable struct TabularQLearningAgent <: AbstractAgent\n",
    "    values::Array{Float64, 2}\n",
    "    α::Float64\n",
    "    γ::Float64\n",
    "    ϵ::Float64\n",
    "    prev_s::Int\n",
    "    action::Int\n",
    "    TabularQLearningAgent(size, num_actions, learning_rate, discount, epsilon) = \n",
    "        new(zeros(size, num_actions), learning_rate, discount, epsilon, 0, 0)\n",
    "end\n",
    "\n",
    "function reset!(agent::TabularQLearningAgent)\n",
    "    agent.values .= randn(size(agent.values)...)\n",
    "end\n",
    "\n",
    "function start!(agent::TabularQLearningAgent, state)\n",
    "#     fill!(agent.values, 0.5)\n",
    "    agent.prev_s = state\n",
    "    agent.action = if rand() > agent.ϵ\n",
    "        findmax(agent.values[state, :])[2]\n",
    "    else\n",
    "        rand(1:size(agent.values)[2])\n",
    "    end\n",
    "    \n",
    "    agent.action\n",
    "end\n",
    "\n",
    "function step!(agent::TabularQLearningAgent, state, rew, term)\n",
    "    \n",
    "    # update action-state values\n",
    "    q = agent.values[agent.prev_s, agent.action]\n",
    "    q_prime = maximum(agent.values[state, :])\n",
    "    \n",
    "    # Notice the difference in update for terminal and not terminal!\n",
    "    δ = if !term\n",
    "        rew + agent.γ * q_prime - q\n",
    "    else\n",
    "        rew - q\n",
    "    end\n",
    "    agent.values[agent.prev_s, agent.action] += agent.α * δ\n",
    "\n",
    "    agent.prev_s = state\n",
    "    agent.action = if rand() > agent.ϵ\n",
    "        findmax(agent.values[state, :])[2]\n",
    "    else\n",
    "        rand(1:size(agent.values)[2])\n",
    "    end\n",
    "    \n",
    "    agent.action\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments/Episodes\n",
    "\n",
    "We've implemented an agent and an environment, now we need to write some code to glue these together to run an episode. The function `run_episode!` is a simplified version of the implementation found in [MinimalRLCore](https://github.com/mkschleg/MinimalRLCore.jl/blob/master/src/episode.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function run_episode!(env, agent)\n",
    "    s = start!(env)\n",
    "    a = start!(agent, s)\n",
    "\n",
    "    total_rews = 0.0\n",
    "    steps = 0\n",
    "    \n",
    "    while !is_terminal(env)\n",
    "        s, r, t = step!(env, a)\n",
    "        a = step!(agent, s, r, t)\n",
    "        total_rews += r\n",
    "        steps += 1\n",
    "    end\n",
    "    total_rews, steps\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are used in the experiment.\n",
    "\n",
    "using Plots\n",
    "using ProgressMeter\n",
    "using Statistics\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function run_experiment(num_runs, num_episodes, seed=1032; markov_chain_size=100, α=0.1, γ=0.9, ϵ=0.01)\n",
    "\n",
    "    Random.seed!(seed)\n",
    "    \n",
    "    env = MarkovChain(markov_chain_size)\n",
    "    agent = TabularQLearningAgent(markov_chain_size, 2, α, γ, ϵ)\n",
    "\n",
    "    reset!(agent)\n",
    "\n",
    "    returns = zeros(num_episodes, num_runs)\n",
    "    steps = zeros(Int, num_episodes, num_runs)\n",
    "\n",
    "    @showprogress 0.1 \"Runs: \" for r in 1:num_runs\n",
    "        reset!(agent)\n",
    "        for i in 1:num_episodes\n",
    "            total_rew, num_steps = run_episode!(env, agent)\n",
    "            returns[i, r] = total_rew\n",
    "            steps[i, r] = num_steps\n",
    "        end\n",
    "    end\n",
    "    plot(mean(returns; dims=2), ribbon=std(returns; dims=2)./sqrt(num_runs), legend=false)\n",
    "end\n",
    "run_experiment(100, 500; ϵ=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed the long runtime after the progress bar seemed complete. This has to do w/ one of the main issues w/ Julia (or the time-to-first-plot issue). What is happening is a bunch of code in the `plot` function is having to be compiled (plotting is really complicated). While annoying, after subsequent evaluations of the cell you can see that plotting is now fast. While out of scope for this document, there are a few fixes for this issue with the recommended issue being [PackageCompiler.jl](https://github.com/JuliaLang/PackageCompiler.jl). The way this works is by precompiling packages to put into the stdlib (much like those packages found in Base), and will significantly speed up the first run of `plot`.\n",
    "\n",
    "Now you have a function encapsulating your entire experiment to play around with! This is a pattern you will see often in Julia. Instead of scripting (like in python) you will want to wrap pieces of code into functions so the compiler can work its optimization magic on it. This will also make it so you can use the repl to test code and re-run code much more easily (of course using tools like [Revise](https://timholy.github.io/Revise.jl/stable/) which we'll talk about in time). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Interface\n",
    "\n",
    "Another populare interface is that provided by [OpenAI Gym](https://gym.openai.com). While I don't use this interface, it is still very possible to define and use within Julia. My preference is not any indication on the usefullness of the interface, but is an indication of me learning RL using the RL Glue interface. The Gym interface seems to have been widely adopted in the Python RL community, and many environment packages follow this API.\n",
    "\n",
    "While not provided in this tutorial, it should be clear how to implement the Gym API. It is also possible to extend any environment to either interface thanks to multiple dispatch!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
